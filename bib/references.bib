@inproceedings{berti2024pm,
  author = {Berti, Alessandro and Kourani, Humam and van der Aalst, Wil MP},
  booktitle = {International Conference on Process Mining},
  doi = {10.1007/978-3-031-82225-4_45},
  organization = {Springer},
  pages = {610--623},
  publisher = {Springer},
  title = {{PM}-{LLM}-Benchmark: Evaluating large language models on process mining tasks},
  year = {2024},
  abstract = {Large Language Models (LLMs) have the potential to semi-automate some process mining (PM) analyses. While commercial models are already adequate for many analytics tasks, the competitive level of open-source LLMs in PM tasks is unknown. In this paper, we propose PM-LLM-Benchmark, the first comprehensive benchmark for PM focusing on domain knowledge (process-mining-specific and process-specific) and on different implementation strategies. We focus also on the challenges in creating such a benchmark, related to the public availability of the data and on evaluation biases by the LLMs. Overall, we observe that most of the considered LLMs can perform some process mining tasks at a satisfactory level, but tiny models that would run on edge devices are still inadequate. We also conclude that while the proposed benchmark is useful for identifying LLMs that are adequate for process mining tasks, further research is needed to overcome the evaluation biases and perform a more thorough ranking of the “competitive” LLMs.}
}


@inproceedings{chen2024sheetagent,
  author = {Chen, Yibin and Yuan, Yifu and Zhang, Zeyu and Zheng, Yan and Liu, Jinyi and Ni, Fei and Hao, Jianye},
  booktitle = {ICML 2024 Workshop on LLMs and Cognition},
  doi = {10.1145/3696410.3714962},
  pages = {158 - 177},
  publisher = {ACM},
  title = {{SheetAgent}: A generalist agent for spreadsheet reasoning and manipulation via large language models},
  year = {2024},
  abstract = {Spreadsheets are ubiquitous across the World Wide Web, playing a critical role in enhancing work efficiency across various domains. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce SheetRM, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose SheetAgent, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: Planner, Informer, and Retriever, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20--40% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at the https://sheetagent.github.io/. The datasets and source code are available at https://anonymous.4open.science/r/SheetAgent.}
}


@article{kumar2024large,
  author = {Kumar, Pranjal},
  doi = {10.1007/s10462-024-10888-y},
  journal = {Artificial Intelligence Review},
  number = {10},
  pages = {260},
  publisher = {Springer},
  title = {{Large Language Models (LLMs)}: survey, technical frameworks, and future challenges},
  volume = {57},
  year = {2024},
  abstract = {Artificial intelligence (AI) has significantly impacted various fields. Large language models (LLMs) like GPT-4, BARD, PaLM, Megatron-Turing NLG, Jurassic-1 Jumbo etc., have contributed to our understanding and application of AI in these domains, along with natural language processing (NLP) techniques. This work provides a comprehensive overview of LLMs in the context of language modeling, word embeddings, and deep learning. It examines the application of LLMs in diverse fields including text generation, vision-language models, personalized learning, biomedicine, and code generation. The paper offers a detailed introduction and background on LLMs, facilitating a clear understanding of their fundamental ideas and concepts. Key language modeling architectures are also discussed, alongside a survey of recent works employing LLM methods for various downstream tasks across different domains. Additionally, it assesses the limitations of current approaches and highlights the need for new methodologies and potential directions for significant advancements in this field.}
}


@article{lu2025large,
  author = {Lu, Weizheng and Zhang, Jing and Fan, Ju and Fu, Zihao and Chen, Yueguo and Du, Xiaoyong},
  doi = {10.1007/s11704-024-40763-6},
  journal = {Frontiers of Computer Science},
  number = {2},
  pages = {192350},
  publisher = {Springer},
  title = {{Large Language Model} for Table Processing: A Survey},
  volume = {19},
  year = {2025},
  abstract = {Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, Web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.}
}


@inproceedings{padmanabhan2010interactive,
  author = {Padmanabhan, Raghav Krishna and Jandhyala, Ramana Chakradhar and Krishnamoorthy, Mukkai and Nagy, George and Seth, Sharad and Silversmith, William},
  booktitle = {Graphics Recognition. Achievements, Challenges, and Evolution: 8th International Workshop, GREC 2009},
  doi = {10.1007/978-3-642-13728-0_3},
  pages = {25--36},
  publisher = {Springer},
  title = {Interactive conversion of web tables},
  year = {2010},
  abstract = {Two hundred web tables from ten sites were imported into Excel. The tables were edited as needed, then converted into layout independent Wang Notation using the Table Abstraction Tool (TAT). The output generated by TAT consists of XML files to be used for constructing narrow-domain ontologies. On an average each table required 104 seconds for editing. Augmentations like aggregates, footnotes, table titles, captions, units and notes were also extracted in an average time of 93 seconds. Every user intervention was logged and audited. The logged interactions were analyzed to determine the relative influence of factors like table size, number of categories and various types of augmentations on the processing time. The analysis suggests which aspects of interactive table processing can be automated in the near term, and how much time such automation would save. The correlation coefficient between predicted and actual processing time was 0.66.}
}


@inproceedings{redis2024skill,
  author = {Redis, Andrei Cosmin and Sani, Mohammadreza Fani and Zarrin, Bahram and Burattin, Andrea},
  booktitle = {International Conference on Process Mining},
  doi = {10.1007/978-3-031-82225-4_48},
  pages = {650--662},
  publisher = {Springer},
  title = {Skill Learning Using Process Mining for Large Language Model Plan Generation},
  year = {2024},
  abstract = {Large language models (LLMs) hold promise for generating plans for complex tasks, but their effectiveness is limited by sequential execution, lack of control flow models, and difficulties in skill retrieval. Addressing these issues is crucial for improving the efficiency and interpretability of plan generation as LLMs become more central to automation and decision-making. We introduce a novel approach to skill learning in LLMs by integrating process mining techniques, leveraging process discovery for skill acquisition, process models for skill storage, and conformance checking for skill retrieval. Our methods enhance text-based plan generation by enabling flexible skill discovery, parallel execution, and improved interpretability. Experimental results suggest the effectiveness of our approach, with our skill retrieval method surpassing state-of-the-art accuracy baselines under specific conditions.}
}


@article{singh2023format5,
  author = {Singh, Mukul and Cambronero, José and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Nouri, Elnaz and Raza, Mohammad and Verbruggen, Gust},
  doi = {10.14778/3632093.3632111},
  journal = {Proceedings of the VLDB Endowment},
  number = {3},
  pages = {497--510},
  publisher = {VLDB Endowment},
  title = {Format5: Abstention and examples for conditional table formatting with natural language},
  volume = {17},
  year = {2023},
  abstract = {Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires understanding and implementing the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.}
}


@incollection{wulff2025natural,
  author = {Wulff, Peter and Kubsch, Marcus and Krist, Christina},
  booktitle = {Applying Machine Learning in Science Education Research: When, How, and Why?},
  pages = {117--142},
  publisher = {Springer Nature Switzerland Cham},
  title = {Natural Language Processing and Large Language Models},
  doi = {10.1007/978-3-031-74227-9_7},
  year = {2025},
  abstract = {In this chapter we introduce the basics of natural language processing techniques that are important to systematically analyze language data. In particular, we will utilize simple large language models and showcase examples of how to apply them in science education research contexts. We will also point to recently advanced large language models that are capable of solving problems without further training, which opens up novel potentials (and challenges) for science education research.}
}


@inproceedings{ye2023large,
  author = {Ye, Yunhu and Hui, Binyuan and Yang, Min and Li, Binhua and Huang, Fei and Li, Yongbin},
  booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  doi = {10.1145/3539618.3591708},
  pages = {174--184},
  publisher = {ACM},
  title = {{Large Language Models} are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning},
  year = {2023},
  abstract = {Table-based reasoning has shown remarkable progress in a wide range of table-based tasks. It is a challenging task, which requires reasoning over both free-form natural language (NL) questions and (semi-)structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on ''huge'' evidence (tables). In addition, most existing methods struggle to reason over complex questions since the essential information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning, and (ii) decompose a complex question into simpler sub-questions for text reasoning. First, we use a powerful LLM to decompose the evidence involved in the current question into the sub-evidence that retains the relevant information and excludes the remaining irrelevant information from the ''huge'' evidence. Second, we propose a novel ''parsing-execution-filling'' strategy to decompose a complex question into simper step-by-step sub-questions by generating intermediate SQL queries as a bridge to produce numerical and logical sub-questions with a powerful LLM. Finally, we leverage the decomposed sub-evidence and sub-questions to get the final answer with a few in-context prompting examples. Extensive experiments on three benchmark datasets (TabFact, WikiTableQuestion, and FetaQA) demonstrate that our method achieves significantly better results than competitive baselines for table-based reasoning. Notably, our method outperforms human performance for the first time on the TabFact dataset. In addition to impressive overall performance, our method also has the advantage of interpretability, where the returned results are to some extent tractable with the generated sub-evidence and sub-questions. For reproducibility, we release our source code and data at: https://github.com/AlibabaResearch/DAMO-ConvAI.}
}


@inproceedings{zhang2014towards,
  author = {Zhang, Ziqi},
  booktitle = {The Semantic Web--ISWC 2014},
  doi = {10.1007/978-3-319-11964-9_31},
  pages = {487--502},
  publisher = {Springer},
  title = {Towards Efficient and Effective Semantic Table Interpretation},
  year = {2014},
  abstract = {This paper describes TableMiner, the first semantic Table Interpretation method that adopts an incremental, mutually recursive and bootstrapping learning approach seeded by automatically selected ‘partial’ data from a table. TableMiner labels columns containing named entity mentions with semantic concepts that best describe data in columns, and disambiguates entity content cells in these columns. TableMiner is able to use various types of contextual information outside tables for Table Interpretation, including semantic markups (e.g., RDFa/microdata annotations) that to the best of our knowledge, have never been used in Natural Language Processing tasks. Evaluation on two datasets shows that compared to two baselines, TableMiner consistently obtains the best performance. In the classification task, it achieves significant improvements of between 0.08 and 0.38 F1 depending on different baseline methods; in the disambiguation task, it outperforms both baselines by between 0.19 and 0.37 in Precision on one dataset, and between 0.02 and 0.03 F1 on the other dataset. Observation also shows that the bootstrapping learning approach adopted by TableMiner can potentially deliver computational savings of between 24 and 60% against classic methods that ‘exhaustively’ processes the entire table content to build features for interpretation.}
}


